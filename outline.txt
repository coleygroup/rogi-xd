general notes:
--------------
- comparing across representations is not obvious rn
- quick lit search on roughness of learned embeddings
    . paper from regina's group on "optimal graph transport for property predictions"
        * claim in fig. 4 is that molecular embeddings are "smoother" using tsne projections
- want to see what people are doing when they talk about smoothness in learned spaces
- "want to do this qualtitative analysis from a more quantitative lens"

models:
-------
- smiles VAE (GT4SD)
- ChemGPT (https://huggingface.co/ncfrey/ChemGPT-1.2B)
- ChemBERTa (https://huggingface.co/DeepChem/ChemBERTa-77M-MLM)
- Attribute masking GIN

tasks
-----
- TDC benchmark tasks

premise:
--------
- that you can use learned embeddings with a top-model and it would work well for chemistry
    . as it does in biology (c.f. proteins)
- are these embeddings actually "smoother" than fingerprints/descriptors?
- implicit assumption in optimization algo's that the space is smooth and we would like to actually 
    investigate

questions:
-----
- for which properties do these latent spaces become smoother than fingerprint/descriptor spaces?

downstream use of these embeddings/representations is optimization tasks / regression